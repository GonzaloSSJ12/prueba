<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Detector de Señas con Entrenamiento</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/knn-classifier"></script>
  <style>
    body {
      display: flex;
      flex-direction: column;
      align-items: center;
      margin: 0;
      padding: 20px;
      background: #f0f0f0;
      font-family: sans-serif;
    }
    #video, #overlay {
      position: absolute;
      top: 20px;
      left: 50%;
      transform: translateX(-50%) scaleX(-1);
      width: 640px;
      height: 480px;
    }
    #video { z-index: 1; }
    #overlay { 
      z-index: 2;
      pointer-events: none;
    }
    #message {
      margin-top: 520px;
      font-size: 1.5em;
      color: #333;
      text-align: center;
      width: 640px;
      white-space: pre-wrap;
    }
    .controls {
      margin: 10px;
      padding: 10px;
      z-index: 3;
    }
    input, button {
      padding: 8px;
      margin: 5px;
      font-size: 1em;
    }
  </style>
</head>
<body>
  <h1>Detector de Señas Entrenables</h1>
  <div class="controls">
    <input type="text" id="gestureName" placeholder="Nombre del gesto">
    <button onclick="startTraining()">Agregar Gesto</button>
    <button onclick="toggleDetection()">Iniciar Detección</button>
  </div>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="overlay"></canvas>
  <div id="message"></div>

  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

  <script>
    let classifier = knnClassifier.create();
    let isTraining = false;
    let isDetecting = false;
    let currentGesture = null;
    let trainingSamples = 0;
    const detectedGestures = [];
    
    const videoElement = document.getElementById('video');
    const canvasElement = document.getElementById('overlay');
    canvasElement.width = 640;
    canvasElement.height = 480;
    const ctx = canvasElement.getContext('2d');
    const messageEl = document.getElementById('message');

    const hands = new Hands({
      locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
    });
    
    hands.setOptions({
      maxNumHands: 1,
      modelComplexity: 1,
      minDetectionConfidence: 0.7,
      minTrackingConfidence: 0.5
    });

    hands.onResults(results => {
      ctx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      ctx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);

      if (results.multiHandLandmarks) {
        results.multiHandLandmarks.forEach(landmarks => {
          window.drawConnectors(ctx, landmarks, window.HAND_CONNECTIONS, 
            {color: '#00FF00', lineWidth: 2});
          window.drawLandmarks(ctx, landmarks, {color: '#FF0000', lineWidth: 1});
        });

        if (isTraining || isDetecting) {
          processGesture(results.multiHandLandmarks[0]);
        }
      }
    });

    const camera = new Camera(videoElement, {
      onFrame: async () => await hands.send({ image: videoElement }),
      width: 640,
      height: 480
    });
    camera.start();

    async function processGesture(landmarks) {
      const features = getNormalizedFeatures(landmarks);
      
      if (isTraining) {
        classifier.addExample(tf.tensor2d(features, [1, 63]), currentGesture);
        trainingSamples++;
        messageEl.textContent = `Entrenando: ${currentGesture} (${trainingSamples}/3)`;
        
        if (trainingSamples >= 3) {
          isTraining = false;
          messageEl.textContent = `${currentGesture} entrenado!`;
          trainingSamples = 0;
        }
      }
      
      if (isDetecting) {
        const result = await classifier.predictClass(tf.tensor2d(features, [1, 63]));
        if (result.confidences[result.label] > 0.7) {
          detectedGestures.push(result.label);
          updateMessage();
        }
      }
    }

    function getNormalizedFeatures(landmarks) {
      const minMax = landmarks.reduce((acc, l) => ({
        minX: Math.min(acc.minX, l.x),
        maxX: Math.max(acc.maxX, l.x),
        minY: Math.min(acc.minY, l.y),
        maxY: Math.max(acc.maxY, l.y)
      }), {minX: 1, maxX: 0, minY: 1, maxY: 0});

      return landmarks.flatMap(l => [
        (l.x - minMax.minX) / (minMax.maxX - minMax.minX),
        (l.y - minMax.minY) / (minMax.maxY - minMax.minY),
        l.z
      ]);
    }

    function startTraining() {
      currentGesture = document.getElementById('gestureName').value.trim();
      if (!currentGesture) return alert('Ingresa un nombre para el gesto');
      
      isTraining = true;
      document.getElementById('gestureName').value = '';
    }

    function toggleDetection() {
      isDetecting = !isDetecting;
      messageEl.textContent = isDetecting ? 
        'Modo detección activo' : 'Modo detección desactivado';
    }

    function updateMessage() {
      messageEl.textContent = detectedGestures.join(' → ');
    }

    // Guardar y cargar modelo (opcional)
    async function saveModel() {
      const dataset = classifier.getClassifierDataset();
      const datasetObj = {};
      Object.keys(dataset).forEach(key => {
        datasetObj[key] = Array.from(dataset[key].dataSync());
      });
      localStorage.setItem('gestureModel', JSON.stringify(datasetObj));
    }

    async function loadModel() {
      const savedModel = localStorage.getItem('gestureModel');
      if (savedModel) {
        const dataset = JSON.parse(savedModel);
        Object.keys(dataset).forEach(key => {
          const tensor = tf.tensor2d(dataset[key], [dataset[key].length / 63, 63]);
          classifier.addDataset(tensor, key);
        });
      }
    }
    
    // Cargar modelo al iniciar
    loadModel();
  </script>
</body>
</html>
